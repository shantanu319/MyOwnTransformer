# GPT2
A basic implementation of the GPT2 Model (adapted from the full encoder-decoder transformer). This is done in a single python file, which also has a training harness so that you can build it on any datasets you like. I used wikitext-2.
